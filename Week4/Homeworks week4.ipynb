{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homeworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base code for basic data we used during the whole week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements - To keed this in order\n",
    "from nltk.corpus import opinion_lexicon\n",
    "import urllib.request, os, gzip\n",
    "import  json\n",
    "import random\n",
    "import numpy # a powerfull module\n",
    "from nltk.corpus import stopwords # We use it to remove the stopwords of the comments since they dont provide relevant info\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # Divide text in sentences and then in words\n",
    "from sklearn.linear_model import LinearRegression # sklearn is a machine learning toolkit (needs numpy, scipy and matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Baby has already been downloaded to ./data/\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "datadir = './data/'\n",
    "\n",
    "def download_data(dataset_name, datadir):\n",
    "    filename = 'reviews_%s_5.json' % dataset_name\n",
    "    filepath = os.path.join(datadir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"Dataset %s has already been downloaded to %s\" % (dataset_name, datadir))\n",
    "    else:\n",
    "        url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/%s.gz' % filename\n",
    "        urllib.request.urlretrieve(url, filepath + \".gz\")\n",
    "        with gzip.open(filepath + \".gz\", 'rb') as fin:\n",
    "            with open(filepath, 'wb') as fout:\n",
    "                fout.write(fin.read())\n",
    "        print(\"Downloaded dataset %s and saved it to %s\" % (dataset_name, datadir))\n",
    "\n",
    "dataset = \"Baby\"\n",
    "download_data(dataset, datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160792 data for dataset Baby\n"
     ]
    }
   ],
   "source": [
    "def  load_data (dataset_name, datadir):\n",
    "    filepath = os.path.join(datadir, 'reviews_%s_5.json' % dataset_name)\n",
    "    if not os.path.exists(filepath):\n",
    "        download_data(dataset_name, datadir)\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:                            # read file line by line\n",
    "            item_hash = hash(line)                # we will use this later for partitioning our data \n",
    "            item = json.loads(line)               # convert JSON string to Python dict\n",
    "            item['hash'] = item_hash              # add hash for identification purposes\n",
    "            data.append(item)\n",
    "    print(\"Loaded %d data for dataset %s\" % (len(data), dataset_name))\n",
    "    return data\n",
    "\n",
    "# load the data...\n",
    "baby = load_data(dataset, datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 96291 training examples, 32244 validation examples, and 32257 test examples\n"
     ]
    }
   ],
   "source": [
    "def partition_train_validation_test(data):\n",
    "    # 60% : modulus is 0, 1, 2, 3, 4, or 5\n",
    "    data_train = [item for item in data if item['hash']%10<=5]  \n",
    "    # 20% : modulus is 6 or 7\n",
    "    data_valid = [item for item in data if item['hash']%10 in [6,7]] \n",
    "    # 20% : modulus is 8 or 9\n",
    "    data_test  = [item for item in data if item['hash']%10 in [8,9]] \n",
    "    return data_train, data_valid, data_test\n",
    "    \n",
    "baby_train, baby_valid, baby_test = partition_train_validation_test(baby)\n",
    "\n",
    "print(\"Now we have\", len(baby_train), \"training examples,\", len(baby_valid),\n",
    "      \"validation examples, and\", len(baby_test), \"test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8571428571428571, 0.0)\n",
      "(0.0, 0.8571428571428571)\n"
     ]
    }
   ],
   "source": [
    "eng_stopwords = set(stopwords.words('english'))\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "def my_tokenize(text):\n",
    "    # split text into lower-case tokens, removing all-punctuation tokens and stopwords\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        #Adds to the array and array with the words in lowercase, we add them if they are not stopwords and there is at least one letter in it\n",
    "        tokens.extend(x for x in word_tokenize(sentence.lower()) #continues down...\n",
    "                      if x not in eng_stopwords and any(i.isalpha() for i in x))# This extends the list by adding elements, it is different from append... see https://stackoverflow.com/questions/252703/difference-between-append-vs-extend-list-methods-in-python \n",
    "    return tokens\n",
    "\n",
    "def pos_neg_fraction(text): # We recieve the raw text\n",
    "    tokens = my_tokenize(text) # We tokenize the text first\n",
    "    count_pos, count_neg = 0, 0\n",
    "    for t in tokens:\n",
    "        if t in positive_words:\n",
    "            count_pos += 1\n",
    "        if t in negative_words:\n",
    "            count_neg += 1\n",
    "    count_all = len(tokens) # this is because we need to be sure there is no 0 len sentence\n",
    "    if count_all != 0:\n",
    "        return count_pos/count_all, count_neg/count_all\n",
    "    else:\n",
    "        return 0., 0.\n",
    "    \n",
    "pos_example = 'This is a good, great, fantastic, amazing, wonderful, super product!!!'\n",
    "neg_example = 'This is a bad, atrocious, terrible, dreadful, awful, abysmal product!!!'\n",
    "print(pos_neg_fraction(pos_example))\n",
    "print(pos_neg_fraction(neg_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found a fraction of 100.000000 % positive words for example 10561\n",
      "{'reviewerID': 'A2FPJGVT01TVVQ', 'asin': 'B000A88JYQ', 'reviewerName': 'S. Broderick \"Sondra\"', 'helpful': [0, 0], 'reviewText': 'work perfect', 'overall': 5.0, 'summary': 'Five Stars', 'unixReviewTime': 1404691200, 'reviewTime': '07 7, 2014', 'hash': -8116255050029596758}\n",
      "We found a fraction of 100.000000 % negative words for example 24850\n",
      "{'reviewerID': 'A1SLEYD29KEUW1', 'asin': 'B000WUD83O', 'reviewerName': 'ABDULLAH AL-FALAH', 'helpful': [0, 0], 'reviewText': 'too noisy', 'overall': 2.0, 'summary': 'Two Stars', 'unixReviewTime': 1404432000, 'reviewTime': '07 4, 2014', 'hash': -8656440585229339815}\n"
     ]
    }
   ],
   "source": [
    "def dataset_to_matrix(data):\n",
    "    # data is a lot of text in {} that has reviwer name..comment...date...etc.. but all identified with text labels\n",
    "    #in that sence data is an unidiminsional array\n",
    "    # the item \"atribute\", we added it to the data to identify the sections of it\n",
    "    return numpy.array([list(pos_neg_fraction(item['reviewText'])) for item in data])\n",
    "# X_train with two columns and as many rows as there are examples in the data set. \n",
    "#The first column contains the fraction of positive words, \n",
    "#while the second column contains the fraction of negative words for each example.\n",
    "X_train = dataset_to_matrix(baby_train)\n",
    "most_pos, most_neg = numpy.argmax(X_train, axis=0) # find maximum ROW (axis 0).. through aaaaallll the data (OMG!)\n",
    "# print the example with the highest fraction of positive words:\n",
    "print(\"We found a fraction of %f %% positive words for example %d\" % \n",
    "      (100.*X_train[most_pos, 0], most_pos))\n",
    "print(baby_train[most_pos])\n",
    "print(\"We found a fraction of %f %% negative words for example %d\" %\n",
    "      (100.*X_train[most_neg, 1], most_neg))\n",
    "print(baby_train[most_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our feature matrix is two-dimensional and has shape (96291, 2)\n",
      "Our target vector is one-dimensional and has shape (96291,)\n"
     ]
    }
   ],
   "source": [
    "def  dataset_to_targets (data):\n",
    "    return numpy.array([item['overall'] for item in data])\n",
    "\n",
    "Y_train = dataset_to_targets(baby_train)\n",
    "print(\"Our feature matrix is two-dimensional and has shape\", X_train.shape) # contains pos,neg fraction\n",
    "print(\"Our target vector is one-dimensional and has shape\", Y_train.shape) # containd sscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient for the fpos variable is 3.25703026268183\n",
      "The coefficient for the fneg variable is -5.593432938808693\n",
      "The intercept is 4.004491648095601\n"
     ]
    }
   ],
   "source": [
    "lreg = LinearRegression().fit(X_train,Y_train)\n",
    "print(\"The coefficient for the fpos variable is\", lreg.coef_[0])\n",
    "print(\"The coefficient for the fneg variable is\", lreg.coef_[1])\n",
    "print(\"The intercept is\", lreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected rating is 4.655898 stars\n",
      "This is the same as 4.655898 stars\n",
      "The expected rating is 2.885805 stars\n",
      "This is the same as 2.885805 stars\n"
     ]
    }
   ],
   "source": [
    "#If the review contains 20% positive words (fpos==0.2) \n",
    "#but still no negative words (fneg==0), we would expect the following rating:\n",
    "features = [[0.2, 0]]\n",
    "expected_rating_A = lreg.predict(features)[0]\n",
    "print(\"The expected rating is %f stars\" % expected_rating_A)\n",
    "# we can also compute this explicitly:\n",
    "expected_rating_B = lreg.intercept_ + 0.2*lreg.coef_[0] + 0*lreg.coef_[1]\n",
    "print(\"This is the same as %f stars\" % expected_rating_B)\n",
    "#However, if the review contains no positive words (fpos==0) but 20% negative words (fneg==0.2),\n",
    "#we expect the following rating:\n",
    "features = [[0, 0.2]]\n",
    "expected_rating_A = lreg.predict(features)[0]\n",
    "print(\"The expected rating is %f stars\" % expected_rating_A)\n",
    "# we can also compute this explicitly:\n",
    "expected_rating_B = lreg.intercept_ + 0 * lreg.coef_[0] + 0.2 * lreg.coef_[1]\n",
    "print(\"This is the same as %f stars\" % expected_rating_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - calculate the prediction for 100% pos, and 100% neg review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected rating for 100 pos review is 7.261522 stars\n",
      "The expected rating  for 100 neg review is -1.588941 stars\n"
     ]
    }
   ],
   "source": [
    "features_100pos=[[1,0]]\n",
    "features_100neg=[[0,1]]\n",
    "expected_rating_pos = lreg.predict(features_100pos)[0] # I think 0 is for linear as in kernel = {‘linear’, ‘rbf’, ‘poly’, ‘sigmoid’, ‘precomputed’}\n",
    "expected_rating_neg = lreg.predict(features_100neg)[0]\n",
    "print(\"The expected rating for 100 pos review is %f stars\" % expected_rating_pos)\n",
    "print(\"The expected rating  for 100 neg review is %f stars\" % expected_rating_neg)\n",
    "# This model needs a threshold to limit the starts between 0 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Repeat this same process for \"Apps for Android\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
